---
title: "Training Random Forest"
author: "Bill Lang"
date: "7/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidygeocoder)
library(ggmap)
library(randomForest)
```


This first half will construct a random forest model on the 15,713 observations that have a full address and an area specification in the top 50.

Reading in file.

```{r}
load(file = "cleanNY.RData")
NYData
```

Removing areas with no information. 

```{r}
NYData <- NYData %>% filter(Column.Location != "") 
```

Finding the top 15 labeled areas.

```{r}
NYData$Column.Location <- tolower(NYData$Column.Location)

#More column.location cleaning here
ny$Column.Location <- trimws(ny$Column.Location, which = c("right"))
ny$Column.Location <- tolower(ny$Column.Location)
ny$Column.Location <- sub("nj","new jersey", ny$Column.Location)
ny$Column.Location <- sub("ny","new york", ny$Column.Location)
ny$Column.Location <- sub("li","long island", ny$Column.Location)
ny$Column.Location <- sub("-","", ny$Column.Location)
ny$Column.Location <- sub("&","", ny$Column.Location)
ny$Column.Location <- sub(" ","", ny$Column.Location)
ny$Column.Location <- sub("county","", ny$Column.Location)
ny$Column.Location <- sub(" state","", ny$Column.Location)
ny$Column.Location <- sub("queens long island","queens", ny$Column.Location)
ny$Column.Location <- sub("queenslongisland","queens", ny$Column.Location)
ny$Column.Location <- sub("queensand long island","queens", ny$Column.Location)
ny$Column.Location <- sub("brooklynlongisland","brooklyn", ny$Column.Location)
ny$Column.Location <- sub(" st","", ny$Column.Location)
ny$Column.Location <- sub("newengland","", ny$Column.Location)
ny$Column.Location <- sub("manhattanbronx","manhattan", ny$Column.Location)
ny$Column.Location <- sub("ville","", ny$Column.Location)
ny$Column.Location <- sub("brooklynqueens","brooklyn", ny$Column.Location)

tbl <- NYData %>% group_by(Column.Location) %>% summarise(count = n()) %>% arrange(desc(count))
listOfAreasNY <- tbl[1:11,]$Column.Location
listOfAreasNY
save(listOfAreasNY, file = "NYAreas.RData")
```

Find the observations with a top 50 area. 

```{r}
NYData <- NYData %>% filter(Column.Location %in% listOfAreasNY) 
```

```{r}
NYData <- NYData %>% filter(!is.na(latitude))
NYData$Column.Location <- as.factor(NYData$Column.Location)
```

Model Building on the first batch

This model is built on a random shuffle of the data and validated by holding out 20% of the dataset. 
 
```{r}
NYData
modelData <- NYData 
set.seed(343)
shuf <- sample(nrow(modelData))
modelData <- modelData[shuf,]
train <- sample(nrow(modelData)*.8)
dataTrain <- modelData[train,]
dataTest <- modelData[-train,]
```


```{r}
bag <- randomForest(Column.Location ~ latitude + longitude + Year + capitalizedPrice, data = dataTrain, ntree = 1000, importance = TRUE)
yhat.rf <- predict(bag, newdata = dataTest)
misclassRate <- mean(yhat.rf != dataTest$Column.Location)
misclassRate
```

```{r}
predicted <- cbind(yhat.rf, dataTest)
predicted
```

We can then map the results using the ggmaps package below to manuely search for any errors.

```{r}
k <- "AIzaSyDNMoGGfgl9f5KtGCnp9BegNb7ZDqPu7gg"
register_google(key = k)
```

```{r}
map <- get_map(location = 'New York', zoom = 8, maptype = "terrain-background", source = 'google', color = 'color')
ggmap(map) + geom_point(data = modelData, mapping = aes(x = longitude, y = latitude, color = factor(Column.Location)), size = 1) +
  theme(legend.position = "none")
```


Full Model

Using all the available data we construct a full model to use for the rest of the analysis. 

```{r}
fullModel <- randomForest(Column.Location ~ latitude + longitude + Year + capitalizedPrice, data = NYData, ntree = 500, mtry = 2, importance = TRUE)
```


























