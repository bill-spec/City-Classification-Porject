---
title: "Split Dallas"
author: "Bill Lang"
date: "2/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidygeocoder)
library(ggmap)
library(randomForest)
```


This is the file to split the Dallas data to before and after 1950.

Reading in file.

```{r}
load(file = "cleanDallas.RData")
dallasData
```

```{r}
dallasData$Area <- tolower(dallasData$Area)
dallasData$iden = c(1:nrow(dallasData))

dallasData1949 = dallasData %>% filter(Year <= 1949)
dallasData1950 = dallasData %>% filter(Year > 1949)

#Finding the top labeled areas and making dataframes of each

tbl1949 <- dallasData1949 %>% group_by(location) %>% summarise(count = n()) %>% arrange(desc(count)) %>% filter(location != "")
listOfAreas1949 <- tbl1949[1:20,]$location


tbl1950 <- dallasData1950 %>% group_by(location) %>% summarise(count = n()) %>% arrange(desc(count))%>% filter(location != "")
listOfAreas1950 <- tbl1950[1:20,]$location


tbl <- dallasData %>% group_by(location) %>% summarise(count = n()) %>% arrange(desc(count))%>% filter(location != "")
listOfAreas <- tbl[1:20,]$location


dallas1949 <- dallasData1949 %>% filter(location %in% listOfAreas1949)
dallas1950 <- dallasData1950 %>% filter(location %in% listOfAreas1950)
dallasAll <- dallasData %>% filter(location %in% listOfAreas)
```


```{r}
#Train and investigate the results the model on those observations in the top 20 selected 


#Preloading variable lists and vectors
topLocationsList = list(listOfAreas1949,listOfAreas1950)
allDataLists = list(dallasData1949,dallasData1950)

splitCount = c(1:2)

misclassVector = splitCount
modelList = list()


for(i in splitCount){
  
   
  #Take the data and repeat 
  currentDataFrame = allDataLists[[i]] %>% filter(location %in% topLocationsList[[i]])
  currentDataFrame
  
  currentDataFrame$location = as.character(currentDataFrame$location)
  currentDataFrame$location = as.factor(currentDataFrame$location)
  
  
  modelData <- currentDataFrame 
  set.seed(343)
  shuf <- sample(nrow(modelData))
  modelData <- modelData[shuf,]
  train <- sample(nrow(modelData)*.8)
  modelTrain <- modelData[train,]
  modelTest <- modelData[-train,]
  
  
  bag <- randomForest(location ~ latitude + longitude + Year +Total.rooms.estimation , data = modelTrain, ntree = 1000, importance = TRUE)
  yhat.rf <- predict(bag, newdata = modelTest)
  misclassRate <- mean(yhat.rf != modelTest$location)
  misclassVector[i] = misclassRate
  
k <- "AIzaSyDNMoGGfgl9f5KtGCnp9BegNb7ZDqPu7gg"
register_google(key = k)

map <- get_map(location = 'Dallas', zoom = 10, maptype = "terrain-background", source = 'google', color = 'color')
ggmap(map) + geom_point(data = modelData, mapping = aes(x = longitude, y = latitude, color = factor(location)), size = 0.1) + theme(legend.position = "none")
  

#Train the full models

modelList[[i]] = randomForest(location ~ latitude + longitude + Year +Total.rooms.estimation , data = modelData, ntree = 1000, mtry = 2, importance = TRUE)


}

misclassVector[1]; misclassVector[2]
modelList


```






```{r}
#Take the full models and geocode those dataseets and take the results not in them

predictedList = list()

for(i in splitCount){
  
  #load the data that has yet to have and adequete location
  currentDataFrame = allDataLists[[i]] %>% filter(!(location %in% topLocationsList[[i]]))
  
  yhat.rf <- predict(modelList[[i]], newdata = currentDataFrame)
  predicted <- cbind(currentDataFrame,yhat.rf)
  predicted <- predicted %>% mutate(calculatedColumn = yhat.rf) %>% select(-yhat.rf)
  predictedList[[i]] = predicted
  
  
}



for(i in splitCount){

  if(i == 1){
    
    fullDataSet = predictedList[[1]]
    
  }else{
  
fullDataSet = rbind(fullDataSet, predictedList[[i]]) 
fullDataSet = fullDataSet %>% arrange(iden)

  }
}


for(i in splitCount){

  currentDataFrame = allDataLists[[i]] %>% filter(location %in% topLocationsList[[i]])
  currentDataFrame$calculatedColumn = "Given"
  
fullDataSet = rbind(fullDataSet, currentDataFrame) 
fullDataSet = fullDataSet %>% arrange(iden)


}

fullDataSet

```

```{r}

  k <- "AIzaSyDNMoGGfgl9f5KtGCnp9BegNb7ZDqPu7gg"
register_google(key = k)
map <- get_map(location = 'Dallas', zoom = 10, maptype = "terrain-background", source = 'google', color = 'color')


#want to plot out the predicted for each period
mapList = list()

for(i in splitCount){
  

mapList[[i]] = ggmap(map) + geom_point(data = predictedList[[i]], mapping = aes(x = longitude, y = latitude, color = factor(calculatedColumn)), size = 0.1) + theme(legend.position = "none")

  

}
mapList[[1]]
mapList[[2]]
```


